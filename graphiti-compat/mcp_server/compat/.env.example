# Graphiti MCP OpenAI Compatible Version Environment Variables

# ⚠️ This compatible version does not support the original OPENAI_BASE_URL, OPENAI_API_KEY, and MODEL_NAME configurations. You need to use the new LLM_BASE_URL, LLM_API_KEY, and LLM_MODEL_NAME configurations instead.
# ⚠️ 该兼容版本不支持原 OPENAI_API_KEY 和 MODEL_NAME 配置，需要使用新的 LLM_BASE_URL, LLM_API_KEY, LLM_MODEL_NAME 配置

# ⚠️ This compatible version separates the embedding model configuration, requiring the use of new EMBEDDING_BASE_URL, EMBEDDING_API_KEY, and EMBEDDING_MODEL_NAME configurations.
# ⚠️ 该兼容版本分离了 embedding 模型配置，需要使用新的 EMBEDDING_BASE_URL, EMBEDDING_API_KEY, EMBEDDING_MODEL_NAME 配置

# ⚠️ This compatible version does not support Azure OpenAI, so all related configurations are not supported.
# ⚠️ 该兼容版本不支持 Azure OpenAI，所以所有相关配置均不支持


# MCP service port, default is 8000
# MCP 服务端口，默认为 8000
PORT=8000

# LLM Configuration
# 大模型配置

# LLM request url, note that it is the baseURL without paths such as chat/completion
# 大模型请求地址，注意是不带 chat/completion 等路径的 baseURL
LLM_BASE_URL=https://api.deepseek.com

# LLM API key
# 大模型 API Key
LLM_API_KEY=sk-...

# LLM model name, Generally, it can be found in the API documentation.
# 大模型名称，一般可以通过 API 文档查询到
LLM_MODEL_NAME=deepseek-chat

# Small model name. If the service provider does not have a model with smaller parameters (such as DeepSeek), this variable does not need to be set. If it is not set or left empty, it will fall back to the main model for simple task processing.
# 小模型名称，如果服务商没有参数较小的模型(如 DeepSeek)，可以不设置此变量，不设置或为空时会回退到主模型进行简单任务处理
LLM_SMALL_MODEL_NAME=

# LLM temperature: Generally, the higher the temperature, the greater the randomness and creativity of the responses, but the lower the accuracy. Default is 0.0.
# 大模型温度，一般温度越高，回答的随机性越大，扩展性越好，但准确度越低。默认为 0.0
LLM_TEMPERATURE=0.0

# Embedding Configuration
# Embedding 配置

# Embedding request url, note that it is the baseURL without paths such as /embeddings
# Embedding 请求地址，注意是不带 /embeddings 等路径的 baseURL

# If you are running the MCP service with Docker and using the Ollama local model, you need to use host.docker.internal to access the host machine.
# 如果用 Docker 运行 MCP 服务，并且使用 Ollama 本地模型，需要使用 host.docker.internal 访问宿主机
EMBEDDING_BASE_URL=http://host.docker.internal:11434/v1
# EMBEDDING_BASE_URL=http://localhost:11434/v1

# Embedding API key, if using a local model, it can be left empty or not set
# Embedding API Key，如果使用本地模型，可以为空或不设置
EMBEDDING_API_KEY=

# Embedding model name, generally can be found in the API documentation or ollama model Readme document
# Embedding 模型名称，一般可以通过 API 文档查询到（或 Ollama 模型的 Readme 文档）
EMBEDDING_MODEL_NAME=nomic-embed-text

# Other configurations remain the same as the original version
# 其他配置与原版保持一致，如 LLM 限速
SEMAPHORE_LIMIT=10
